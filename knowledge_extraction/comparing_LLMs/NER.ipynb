{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import json\n",
    "\n",
    "wikiann = load_dataset('wikiann', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'BERT': 'dslim/bert-base-NER',\n",
    "    'DistilBERT': 'dslim/distilbert-NER',\n",
    "    'RoBERTa1': 'MMG/roberta-base-ner-english',\n",
    "    'RoBERTa2': '51la5/roberta-large-NER',\n",
    "    'RoBERTa3': 'FacebookAI/xlm-roberta-large-finetuned-conll03-english',\n",
    "    'ALBERT1': 'ArBert/albert-base-v2-finetuned-ner',\n",
    "    'ALBERT2': 'Jorgeutd/albert-base-v2-finetuned-ner',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    '0': 0,\n",
    "    'B-PER': 1,\n",
    "    'I-PER': 2,\n",
    "    'B-ORG': 3,\n",
    "    'I-ORG': 4,\n",
    "    'B-LOC': 5,\n",
    "    'I-LOC': 6,\n",
    "\n",
    "    'PER' : 1,\n",
    "    'ORG': 3,\n",
    "    'LOC': 5\n",
    "}\n",
    "\n",
    "reverse_label_map = {\n",
    "    '0': '0',\n",
    "    '1': 'B-PER',\n",
    "    '2': 'I-PER',\n",
    "    '3': 'B-ORG',\n",
    "    '4': 'I-ORG',\n",
    "    '5': 'B-LOC',\n",
    "    '6': 'I-LOC',\n",
    "    '7': 'MISC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def merge_result(entities, name):\n",
    "    merged_entities = []\n",
    "    current = None\n",
    "\n",
    "    if name in ['dslim/bert-base-NER', 'dslim/distilbert-NER']:\n",
    "        for entity in entities:\n",
    "            if current == None:\n",
    "                current = entity\n",
    "            else:\n",
    "                if entity['word'].startswith('##'):\n",
    "                    current['word'] += entity['word'][2:]\n",
    "                    current['end'] = entity['end']\n",
    "                    current['score'] = min(current['score'], entity['score'])\n",
    "                else:\n",
    "                    merged_entities.append(current)\n",
    "                    current = entity\n",
    "        if current is not None:\n",
    "            merged_entities.append(current)\n",
    "    else: \n",
    "        symbol = '▁'\n",
    "        if name in ['MMG/roberta-base-ner-english']:\n",
    "            symbol = 'Ġ'\n",
    "\n",
    "        for entity in entities:\n",
    "            if current == None:\n",
    "                current = entity\n",
    "            else:\n",
    "                if not entity['word'].startswith(symbol):\n",
    "                    current['word'] += entity['word']\n",
    "                    current['end'] = entity['end']\n",
    "                    current['score'] = min(current['score'], entity['score'])\n",
    "                else:\n",
    "                    current['word'] = current['word'][1:]\n",
    "                    merged_entities.append(current)\n",
    "                    current = entity\n",
    "        if current is not None:\n",
    "            current['word'] = current['word'][1:]\n",
    "            merged_entities.append(current)\n",
    "    print(merged_entities)    \n",
    "    return merged_entities\n",
    "\n",
    "def find_word_indices(word, tokens):\n",
    "    indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower().strip() == word.lower().strip():\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def get_predicted_tags(results, tokens):\n",
    "    predicted_tags = [0 for _ in range(len(tokens))]\n",
    "    for result in results:\n",
    "        entity = result['entity']\n",
    "        if 'LABEL' in entity:\n",
    "            entity = reverse_label_map[entity[-1]]\n",
    "        if entity in label_map.keys(): # Ignore miscellaneous tags (not labeled in wikidata)\n",
    "            word = result['word']\n",
    "            indices = find_word_indices(word, tokens)\n",
    "            for index in indices:\n",
    "                predicted_tags[index] = label_map[entity]\n",
    "    \n",
    "    return predicted_tags\n",
    "\n",
    "def calculate_metrics(tags_pred, tags_gold):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "    for pred, gold in zip(tags_pred, tags_gold):\n",
    "        if pred == gold and gold != '0':\n",
    "            tp += 1\n",
    "        elif gold != '0' and pred != gold:\n",
    "            fn += 1\n",
    "        elif gold == '0' and pred != '0':\n",
    "            fp += 1\n",
    "        elif gold == '0' and pred == '0':\n",
    "            tn += 1\n",
    "    \n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def test_model(model_name, wiki_dataset, length=-1):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    ner_pipeline = pipeline('ner', model=model_name, tokenizer=tokenizer)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    tokens_all = wiki_dataset['tokens']\n",
    "    tags_all = wiki_dataset['ner_tags']\n",
    "\n",
    "    iterations = len(tokens_all)\n",
    "    if length > 0:\n",
    "        iterations = length\n",
    "\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    for i in range(iterations):\n",
    "        tokens = tokens_all[i]\n",
    "        true_tags = tags_all[i]\n",
    "        text = join_tokens(tokens)\n",
    "        result = ner_pipeline(text)\n",
    "        merged_result = merge_result(result)\n",
    "        predicted_tags = get_predicted_tags(merged_result, tokens)\n",
    "\n",
    "        tp, fp, tn, fn = calculate_metrics(predicted_tags, true_tags)\n",
    "\n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        TN += tn\n",
    "        FN += fn\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    accuracy = TP/(TP + FP + TN + FN) if (TP + FP + TN + FN) else 0\n",
    "    precision = TP/(TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP/(TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return duration, accuracy, precision, recall, f1_score\n",
    "\n",
    "data = wikiann['test'][:]\n",
    "results = {name: {} for name in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens', 'ner_tags', 'langs', 'spans'])\n",
      "Charlie Smith ( Romani poet ) ( 1956–2005 )\n",
      "Currently testing dslim/bert-base-NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9997104, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}, {'entity': 'I-PER', 'score': 0.9997342, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}, {'entity': 'B-MISC', 'score': 0.9994331, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}]\n",
      "{'entity': 'B-PER', 'score': 0.9997104, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}\n",
      "{'entity': 'I-PER', 'score': 0.9997342, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}\n",
      "{'entity': 'B-MISC', 'score': 0.9994331, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}\n",
      "[1, 2, 2, 2, 2, 2, 0, 0, 0]\n",
      "[1, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "Currently testing dslim/distilbert-NER\n",
      "[{'entity': 'B-PER', 'score': 0.99832803, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}, {'entity': 'I-PER', 'score': 0.99822754, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}, {'entity': 'B-MISC', 'score': 0.98910064, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}]\n",
      "{'entity': 'B-PER', 'score': 0.99832803, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}\n",
      "{'entity': 'I-PER', 'score': 0.99822754, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}\n",
      "{'entity': 'B-MISC', 'score': 0.98910064, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}\n",
      "[1, 2, 2, 2, 2, 2, 0, 0, 0]\n",
      "[1, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "Currently testing MMG/roberta-base-ner-english\n",
      "[{'entity': 'B-PER', 'score': 0.9996617, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}, {'entity': 'I-PER', 'score': 0.9995448, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}, {'entity': 'B-MISC', 'score': 0.97795576, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}]\n",
      "{'entity': 'B-PER', 'score': 0.9996617, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}\n",
      "{'entity': 'I-PER', 'score': 0.9995448, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}\n",
      "{'entity': 'B-MISC', 'score': 0.97795576, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}\n",
      "[1, 2, 2, 2, 2, 2, 0, 0, 0]\n",
      "[1, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "Currently testing 51la5/roberta-large-NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiril/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at 51la5/roberta-large-NER were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99999714, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}, {'entity': 'I-PER', 'score': 0.9999976, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}, {'entity': 'I-MISC', 'score': 0.9997234, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}]\n",
      "{'entity': 'I-PER', 'score': 0.99999714, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}\n",
      "{'entity': 'I-PER', 'score': 0.9999976, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}\n",
      "{'entity': 'I-MISC', 'score': 0.9997234, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}\n",
      "[1, 2, 2, 2, 2, 2, 0, 0, 0]\n",
      "[2, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "Currently testing FacebookAI/xlm-roberta-large-finetuned-conll03-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99999714, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}, {'entity': 'I-PER', 'score': 0.9999976, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}, {'entity': 'I-MISC', 'score': 0.9997234, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}]\n",
      "{'entity': 'I-PER', 'score': 0.99999714, 'index': 1, 'word': 'Charlie', 'start': 0, 'end': 7}\n",
      "{'entity': 'I-PER', 'score': 0.9999976, 'index': 2, 'word': 'Smith', 'start': 8, 'end': 13}\n",
      "{'entity': 'I-MISC', 'score': 0.9997234, 'index': 4, 'word': 'Romani', 'start': 16, 'end': 22}\n",
      "[1, 2, 2, 2, 2, 2, 0, 0, 0]\n",
      "[2, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "Currently testing ArBert/albert-base-v2-finetuned-ner\n",
      "[{'entity': 'LABEL_1', 'score': 0.9974179, 'index': 1, 'word': 'charlie', 'start': 0, 'end': 7}, {'entity': 'LABEL_2', 'score': 0.9959001, 'index': 2, 'word': 'smith', 'start': 8, 'end': 13}, {'entity': 'LABEL_0', 'score': 0.99989724, 'index': 3, 'word': '(', 'start': 14, 'end': 15}, {'entity': 'LABEL_7', 'score': 0.9889637, 'index': 5, 'word': 'romani', 'start': 16, 'end': 22}, {'entity': 'LABEL_0', 'score': 0.9976174, 'index': 7, 'word': 'poet', 'start': 23, 'end': 27}, {'entity': 'LABEL_0', 'score': 0.999752, 'index': 8, 'word': ')', 'start': 28, 'end': 29}, {'entity': 'LABEL_0', 'score': 0.99996233, 'index': 10, 'word': '(', 'start': 30, 'end': 31}, {'entity': 'LABEL_0', 'score': 0.99976605, 'index': 12, 'word': '1956–2005', 'start': 32, 'end': 41}, {'entity': 'LABEL_0', 'score': 0.99995434, 'index': 15, 'word': ')', 'start': 42, 'end': 43}]\n",
      "{'entity': 'LABEL_1', 'score': 0.9974179, 'index': 1, 'word': 'charlie', 'start': 0, 'end': 7}\n",
      "{'entity': 'LABEL_2', 'score': 0.9959001, 'index': 2, 'word': 'smith', 'start': 8, 'end': 13}\n",
      "{'entity': 'LABEL_0', 'score': 0.99989724, 'index': 3, 'word': '(', 'start': 14, 'end': 15}\n",
      "{'entity': 'LABEL_7', 'score': 0.9889637, 'index': 5, 'word': 'romani', 'start': 16, 'end': 22}\n",
      "{'entity': 'LABEL_0', 'score': 0.9976174, 'index': 7, 'word': 'poet', 'start': 23, 'end': 27}\n",
      "{'entity': 'LABEL_0', 'score': 0.999752, 'index': 8, 'word': ')', 'start': 28, 'end': 29}\n",
      "{'entity': 'LABEL_0', 'score': 0.99996233, 'index': 10, 'word': '(', 'start': 30, 'end': 31}\n",
      "{'entity': 'LABEL_0', 'score': 0.99976605, 'index': 12, 'word': '1956–2005', 'start': 32, 'end': 41}\n",
      "{'entity': 'LABEL_0', 'score': 0.99995434, 'index': 15, 'word': ')', 'start': 42, 'end': 43}\n",
      "[1, 2, 2, 2, 2, 2, 0, 0, 0]\n",
      "[1, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "Currently testing Jorgeutd/albert-base-v2-finetuned-ner\n",
      "[{'entity': 'B-PER', 'score': 0.99827564, 'index': 1, 'word': 'charlie', 'start': 0, 'end': 7}, {'entity': 'I-PER', 'score': 0.98897994, 'index': 2, 'word': 'smith', 'start': 8, 'end': 13}, {'entity': 'B-MISC', 'score': 0.9974935, 'index': 5, 'word': 'romani', 'start': 16, 'end': 22}]\n",
      "{'entity': 'B-PER', 'score': 0.99827564, 'index': 1, 'word': 'charlie', 'start': 0, 'end': 7}\n",
      "{'entity': 'I-PER', 'score': 0.98897994, 'index': 2, 'word': 'smith', 'start': 8, 'end': 13}\n",
      "{'entity': 'B-MISC', 'score': 0.9974935, 'index': 5, 'word': 'romani', 'start': 16, 'end': 22}\n",
      "[1, 2, 2, 2, 2, 2, 0, 0, 0]\n",
      "[1, 2, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data.keys())\n",
    "i = 520\n",
    "tokens = data['tokens'][i]\n",
    "spans = data['spans'][i]\n",
    "tags = data['ner_tags'][i]\n",
    "text = join_tokens(tokens)\n",
    "\n",
    "print(text)\n",
    "\n",
    "for name in models:\n",
    "    model_name = models[name]\n",
    "    print('Currently testing', model_name)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    ner_pipeline = pipeline('ner', model=model_name, tokenizer=tokenizer)\n",
    "    result = ner_pipeline(text)\n",
    "    merged_result = merge_result(result, model_name)\n",
    "\n",
    "    predicted_tags = get_predicted_tags(merged_result, tokens)\n",
    "    for r in merged_result:\n",
    "        print(r)\n",
    "    print(tags)\n",
    "    print(predicted_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
