{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative LLMs:\n",
    "- LLAMA\n",
    "- Mistral\n",
    "- GPT-3.5 Turbo\n",
    "\n",
    "Non-generative LLMs:\n",
    "- BERT\n",
    "- DistilBERT\n",
    "- RoBERTa\n",
    "- ALBERT\n",
    "- XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# From huggingface\n",
    "models = {\n",
    "    'BERT': 'google-bert/bert-base-uncased',\n",
    "    'DistilBERT': 'distilbert/distilbert-base-uncased',\n",
    "    'AlBERTa': 'albert/albert-base-v2',\n",
    "    'RoBERTa': 'deepset/roberta-base-squad2',\n",
    "    'T5': 'google-t5/t5-base',\n",
    "    'XLNet': 'xlnet/xlnet-base-cased'\n",
    "}\n",
    "tokenizer = models['BERT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/tiril/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/tiril/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "# Introduction to pipeline from transfromers library\n",
    "from transformers import pipeline\n",
    "\n",
    "sentences = ['Kant was a remarkable figure',\n",
    "             'Water is made of hydrogen and oxygen',\n",
    "             'The capital of Norway is Oslo',\n",
    "             'We hope you don\\'t hate it']\n",
    "\n",
    "model = models['BERT']\n",
    "\n",
    "# Sentiment analysis\n",
    "classifier = pipeline('sentiment-analysis', model=model)\n",
    "'''for sentence in sentences:\n",
    "    print(sentence, classifier(sentence))'''\n",
    "\n",
    "\n",
    "# Zero-shot classification\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "'''for sentence in sentences:\n",
    "    print(sentence, classifier(sentence, candidate_labels=['education', 'business', 'science', 'politics']))'''\n",
    "\n",
    "# Generation\n",
    "generator = pipeline('text-generation', model=model)\n",
    "'''generator(\n",
    "    'I am a god. You are merely', \n",
    "    max_length=40, \n",
    "    num_return_sequences=5\n",
    ")'''\n",
    "\n",
    "# Fill masks\n",
    "sentence = 'The SMRs face a common issue, as <mask> they will be expensive, but the market is wide enough to trigger manufacturing processes to lower the <mask>'\n",
    "unmasker = pipeline('fill-mask', top_k=3)\n",
    "#unmasker(sentence)\n",
    "\n",
    "# NER\n",
    "ner = pipeline('ner', grouped_entities=True, model=model)\n",
    "ner('My name is Tiril Mageli and I am an MSc student at Imperial College London in the department of Computing, which is in London')\n",
    "\n",
    "# Question Answering\n",
    "qa = pipeline('question-answering')\n",
    "'''qa(\n",
    "    question='whats the weather like today?',\n",
    "    context='My name is Tiril Mageli and I am an MSc student at Imperial College London in the department of Computing, which is in London'\n",
    ")'''\n",
    "\n",
    "# Summariser\n",
    "text = [\"The six small modular reactor (SMR) developers shortlisted in Great British Nuclear’s (GBN’s) competition now have an extra two weeks to submit documentation due to the General Election.\",\n",
    "    \"The deadline for submitting project documentation has been pushed back from 24 June to 8 July. GBN said no further details were able to be shared due to restrictions on government communications during the pre-election period.\",\n",
    "    \"Energy business publication\",\n",
    "    \"reported that a request for the delay had come from one of the four US-based prospective SMR firms.\",\n",
    "    \"in October 2023 for government support to deliver a new wave of nuclear reactors are EDF Energy, GE-Hitachi Nuclear Energy International, Holtec Britain, NuScale Power, Rolls-Royce SMR and Westinghouse Electric Company UK.\",\n",
    "    \"Of those, GE-Hitachi Nuclear Energy International LLC, Holtec Britain Limited, NuScale Power and Westinghouse Electric Company UK Limited have American parent or partner companies.\",\n",
    "    \"The competition winner will receive government backing to deploy a fleet of SMRs in the UK. At the time of the competition announcement, GBN chief executive Gwyn Parry-Jones said parties would be “aiming for a final contract agreement in the summer”.\",\n",
    "    \"SMRs are nuclear power stations that have lower capacity than large-scale nuclear plantsusually in the 300MW to 500MW range. They are in theory quicker and cheaper to deploy and their construction will be easily repeatable thanks to their modular design, which will see parts created in a factory.\",\n",
    "    \"Even if not successful in GBN’s competition, many of the shortlisted firms have signalled intent to deliver SMRs in the UK.\",\n",
    "    \"announced a prototype module testing facility at the University of Sheffield\",\n",
    "    \"Holtec has shortlisted four UK sites for its SMR module factory\",\n",
    "    \". Westinghouse has plans to\",\n",
    "    \"deploy the first privately funded SMRs in North Teesside by the 2030s\",\n",
    "    \"At the time of the GBN competition announcement, energy security secretary Claire Coutinho said: “Small modular reactors will help the UK rapidly expand nuclear power; deliver cheaper, cleaner and more secure energy for British families and businesses; create well-paid, high-skilled jobs; and grow the economy.\",\n",
    "    \"“This competition has attracted designs from around the world and puts the UK at the front of the global race to develop this exciting, cutting-edge technology and cement our position as a world leader in nuclear innovation.”\",\n",
    "    \"There have been some doubts cast, with\",\n",
    "    \"the Environmental Audit Committee claiming that SMRs will not be able to help the UK decarbonise by 2035\",\n",
    "    \"Additionally, US think tank Institute for Energy Economics and Financial Analysis (IEEFA) has said that SMRs are “too expensive, too slow, and too risky”.\"]\n",
    "full_text = ' '.join(text)\n",
    "\n",
    "summariser = pipeline('summarization', model=model)\n",
    "#summariser('My name is Tiril and I am happy')\n",
    "\n",
    "# Translation\n",
    "#translator = pipeline('translation', model='Helsinki-NLP/opus-mt-fr-en', max_length=100)\n",
    "#translator('Je suis une banane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to AutoModel from transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'albert/albert-base-v2'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "#classifier('I hate mondays')\n",
    "\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "# Loading models\n",
    "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model = AutoModel.from_pretrained('gpt2')\n",
    "#print(type(gpt_model))\n",
    "\n",
    "bart_model = AutoModel.from_pretrained('facebook/bart-base')\n",
    "#print(type(bart_model))\n",
    "\n",
    "# It is also possible to download simply the configuration (ie. random weights)\n",
    "bert_config = AutoConfig.from_pretrained('bert-base-cased')\n",
    "print(type(bert_config))\n",
    "\n",
    "# Or from a specific checkpoint\n",
    "from transformers import BertConfig\n",
    "bert_config = BertConfig.from_pretrained('bert-base-cased')\n",
    "print(type(bert_config))\n",
    "print(bert_config) # we can change any of these if we want\n",
    "\n",
    "# E.g. 10 layers instead of 12\n",
    "from transformers import BertModel\n",
    "bert_config = BertConfig.from_pretrained('bert-base-cased', num_hidden_layers=10)\n",
    "bert_model = BertModel(bert_config)\n",
    "print(bert_model)\n",
    "\n",
    "# Saving a model\n",
    "bert_model.save_pretrained('my-bert-model') #saves in current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 151, 39487, 39618, 10107, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "Tokens: ['[CLS]', 'i', 'hate', 'monday', '##s', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The AutoModelForSequenceClassification and AutoTokenizer classes work together\n",
    "to power the pipeline() used above. An AutoClass is a shortcut that automatically \n",
    "retrieves the architecture of a pretrained model.T_destination\n",
    "\n",
    "A tokenizer is responsible for preprocessing text into an array of numbers as inputs \n",
    "to a model, inluding rules for splitting at which level of word. Important: make sure\n",
    "you use the SAME tokenizer as the model was pretrained on to get good results.\n",
    "'''\n",
    "\n",
    "# Loading tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoding = tokenizer('I hate Mondays')\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "print(encoding)\n",
    "print('Tokens:', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SQuAD data (NOT NECESSARY)\n",
    "import requests\n",
    "import json \n",
    "\n",
    "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "with open('squad-dev-v2.0.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing: BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0xa948c06a0>\n",
      "Currently testing: DistilBERT\n",
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0xa97f0fa30>\n",
      "Currently testing: RoBERTa\n",
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0xa948c06a0>\n"
     ]
    }
   ],
   "source": [
    "# SQuAD test\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from datasets import load_dataset\n",
    "import difflib\n",
    "import time\n",
    "\n",
    "squad_models = {\n",
    "    'BERT': 'google-bert/bert-large-uncased-whole-word-masking-finetuned-squad',\n",
    "    'DistilBERT': 'distilbert/distilbert-base-cased-distilled-squad',\n",
    "    'RoBERTa': 'deepset/roberta-base-squad2',\n",
    "    #'TinyLLAMA': 'TinyLlama/TinyLlama-1.1B-step-50K-105b',\n",
    "    #'Mistral': 'mistralai/Mistral-7B-v0.1'\n",
    "}\n",
    "\n",
    "squad = load_dataset('squad')\n",
    "\n",
    "def is_close_match(predicted, gold_answers, threshold=0.99):\n",
    "    for gold in gold_answers:\n",
    "        similarity = difflib.SequenceMatcher(None, predicted, gold).ratio()\n",
    "        if similarity >= threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "results = {name: {} for name in squad_models}\n",
    "for name in squad_models:\n",
    "    print('Currently testing:', name)\n",
    "\n",
    "    model_name = squad_models[name]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "    print(qa_pipeline)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    for example in squad['validation']:\n",
    "        question = example['question']\n",
    "        context = example['context']\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        pred = result['answer'].strip().lower()\n",
    "        gold = [g.strip().lower() for g in example['answers']['text']]\n",
    "\n",
    "        if is_close_match(pred, gold):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    end = time.time()\n",
    "    accuracy = correct/total\n",
    "    length = end-start\n",
    "\n",
    "    results[name]['Accuracy'] = accuracy\n",
    "    results[name]['Time'] = length\n",
    "\n",
    "with open('squad_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "\n",
    "wikiann = load_dataset('wikiann', 'en')\n",
    "data = wikiann['test'][:]\n",
    "models = {\n",
    "    'BERT': 'dslim/bert-base-NER',\n",
    "    'DistilBERT': 'dslim/distilbert-NER',\n",
    "    'RoBERTa': 'Jean-Baptiste/roberta-large-ner-english'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens', 'ner_tags', 'langs', 'spans'])\n",
      "Tokens: ['**', 'Michael', 'Somare', ',', 'Prime', 'Minister', 'of', 'Papua', 'New', 'Guinea', '(', '1982–1985', ')']\n",
      "Tags: ['PER: Michael Somare', 'ORG: Prime Minister of Papua New Guinea']\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())\n",
    "i = 1022\n",
    "tokens = data['tokens'][i]\n",
    "spans = data['spans'][i]\n",
    "print('Tokens:', tokens)\n",
    "print('Tags:', spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "'''def join_tokens(tokens):\n",
    "    string = ''\n",
    "    for token in tokens:\n",
    "        if token.isalnum():\n",
    "            string = string + ' ' + token\n",
    "        else:\n",
    "            string = string + token\n",
    "    string = string.strip()\n",
    "    return string'''\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "label_map = {\n",
    "    '0': 0,\n",
    "    'B-PER': 1,\n",
    "    'I-PER': 2,\n",
    "    'B-ORG': 3,\n",
    "    'I-ORG': 4,\n",
    "    \"B-LOC\": 5,\n",
    "    'I-LOC': 6,\n",
    "\n",
    "    'PER' : 1,\n",
    "    'ORG': 3,\n",
    "    'LOC': 5\n",
    "    \n",
    "}\n",
    "\n",
    "def merge_result(entities):\n",
    "    merged_entities = []\n",
    "    current = None\n",
    "\n",
    "    for entity in entities:\n",
    "        if current == None:\n",
    "            current = entity\n",
    "        else:\n",
    "            if entity['word'].startswith('##'):\n",
    "                current['word'] += entity['word'][2:]\n",
    "                current['end'] = entity['end']\n",
    "                current['score'] = min(current['score'], entity['score'])\n",
    "            else:\n",
    "                merged_entities.append(current)\n",
    "                current = entity\n",
    "    \n",
    "    if current is not None:\n",
    "        merged_entities.append(current)\n",
    "    \n",
    "    return merged_entities\n",
    "\n",
    "def find_word_indices(word, tokens):\n",
    "    indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == word:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def get_predicted_tags(results, tokens):\n",
    "    predicted_tags = [0 for _ in range(len(tokens))]\n",
    "    for result in results:\n",
    "        entity = result['entity']\n",
    "        if entity in label_map.keys():\n",
    "            word = result['word']\n",
    "            indices = find_word_indices(word, tokens)\n",
    "            for index in indices:\n",
    "                predicted_tags[index] = label_map[entity]\n",
    "    \n",
    "    return predicted_tags\n",
    "\n",
    "def calculate_metrics(tags_pred, tags_gold):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for pred, gold in zip(tags_pred, tags_gold):\n",
    "        if pred == gold and gold != '0':\n",
    "            tp += 1\n",
    "        elif gold != '0' and pred != gold:\n",
    "            fn += 1\n",
    "        elif gold == '0' and pred != '0':\n",
    "            fp += 1\n",
    "    \n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, wiki_dataset, length=-1):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    ner_pipeline = pipeline('ner', model=model_name, tokenizer=tokenizer)\n",
    "    \n",
    "    print('\\nCurrently testing:', model_name)\n",
    "    start_time = time.time()\n",
    "\n",
    "    tokens_all = wiki_dataset['tokens']\n",
    "    tags_all = wiki_dataset['ner_tags']\n",
    "\n",
    "    iterations = len(tokens_all)\n",
    "    if length > 0:\n",
    "        iterations = length\n",
    "\n",
    "    TP, FP, FN = 0, 0, 0\n",
    "    for i in range(iterations):\n",
    "        tokens = tokens_all[i]\n",
    "        true_tags = tags_all[i]\n",
    "        text = join_tokens(tokens)\n",
    "        result = ner_pipeline(text)\n",
    "        merged_result = merge_result(result)\n",
    "        predicted_tags = get_predicted_tags(merged_result, tokens)\n",
    "\n",
    "        tp, fp, fn = calculate_metrics(predicted_tags, true_tags)\n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        FN += fn\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    precision = TP/(TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP/(TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print('Time:', duration)\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1_score:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(models['RoBERTa'], data, length=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
