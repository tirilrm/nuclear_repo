{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative LLMs:\n",
    "- LLAMA\n",
    "- Mistral\n",
    "- GPT-3.5 Turbo\n",
    "\n",
    "Non-generative LLMs:\n",
    "- BERT\n",
    "- DistilBERT\n",
    "- RoBERTa\n",
    "- ALBERT\n",
    "- XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# From huggingface\n",
    "models = {\n",
    "    'BERT': 'google-bert/bert-base-uncased',\n",
    "    'DistilBERT': 'distilbert/distilbert-base-uncased',\n",
    "    'AlBERTa': 'albert/albert-base-v2'\n",
    "}\n",
    "tokenizer = models['BERT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import sentencepiece\n",
    "\n",
    "sentences = ['Kant was a remarkable figure',\n",
    "             'Water is made of hydrogen and oxygen',\n",
    "             'The capital of Norway is Oslo',\n",
    "             'We hope you don\\'t hate it']\n",
    "\n",
    "\n",
    "# Sentiment analysis\n",
    "classifier = pipeline('sentiment-analysis', model = models['AlBERTa'])\n",
    "'''for sentence in sentences:\n",
    "    print(sentence, classifier(sentence))'''\n",
    "\n",
    "\n",
    "# Zero-shot classification\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "'''for sentence in sentences:\n",
    "    print(sentence, classifier(sentence, candidate_labels=['education', 'business', 'science', 'politics']))'''\n",
    "\n",
    "# Generation\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "'''generator(\n",
    "    'I am a god. You are merely', \n",
    "    max_length=40, \n",
    "    num_return_sequences=5\n",
    ")'''\n",
    "\n",
    "# Fill masks\n",
    "sentence = 'The SMRs face a common issue, as <mask> they will be expensive, but the market is wide enough to trigger manufacturing processes to lower the <mask>'\n",
    "unmasker = pipeline('fill-mask', top_k=3)\n",
    "#unmasker(sentence)\n",
    "\n",
    "# NER\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "ner('My name is Tiril Mageli and I am an MSc student at Imperial College London in the department of Computing, which is in London')\n",
    "\n",
    "# Question Answering\n",
    "qa = pipeline('question-answering')\n",
    "'''qa(\n",
    "    question='whats the weather like today?',\n",
    "    context='My name is Tiril Mageli and I am an MSc student at Imperial College London in the department of Computing, which is in London'\n",
    ")'''\n",
    "\n",
    "# Summariser\n",
    "text = [\"The six small modular reactor (SMR) developers shortlisted in Great British Nuclear’s (GBN’s) competition now have an extra two weeks to submit documentation due to the General Election.\",\n",
    "    \"The deadline for submitting project documentation has been pushed back from 24 June to 8 July. GBN said no further details were able to be shared due to restrictions on government communications during the pre-election period.\",\n",
    "    \"Energy business publication\",\n",
    "    \"reported that a request for the delay had come from one of the four US-based prospective SMR firms.\",\n",
    "    \"in October 2023 for government support to deliver a new wave of nuclear reactors are EDF Energy, GE-Hitachi Nuclear Energy International, Holtec Britain, NuScale Power, Rolls-Royce SMR and Westinghouse Electric Company UK.\",\n",
    "    \"Of those, GE-Hitachi Nuclear Energy International LLC, Holtec Britain Limited, NuScale Power and Westinghouse Electric Company UK Limited have American parent or partner companies.\",\n",
    "    \"The competition winner will receive government backing to deploy a fleet of SMRs in the UK. At the time of the competition announcement, GBN chief executive Gwyn Parry-Jones said parties would be “aiming for a final contract agreement in the summer”.\",\n",
    "    \"SMRs are nuclear power stations that have lower capacity than large-scale nuclear plantsusually in the 300MW to 500MW range. They are in theory quicker and cheaper to deploy and their construction will be easily repeatable thanks to their modular design, which will see parts created in a factory.\",\n",
    "    \"Even if not successful in GBN’s competition, many of the shortlisted firms have signalled intent to deliver SMRs in the UK.\",\n",
    "    \"announced a prototype module testing facility at the University of Sheffield\",\n",
    "    \"Holtec has shortlisted four UK sites for its SMR module factory\",\n",
    "    \". Westinghouse has plans to\",\n",
    "    \"deploy the first privately funded SMRs in North Teesside by the 2030s\",\n",
    "    \"At the time of the GBN competition announcement, energy security secretary Claire Coutinho said: “Small modular reactors will help the UK rapidly expand nuclear power; deliver cheaper, cleaner and more secure energy for British families and businesses; create well-paid, high-skilled jobs; and grow the economy.\",\n",
    "    \"“This competition has attracted designs from around the world and puts the UK at the front of the global race to develop this exciting, cutting-edge technology and cement our position as a world leader in nuclear innovation.”\",\n",
    "    \"There have been some doubts cast, with\",\n",
    "    \"the Environmental Audit Committee claiming that SMRs will not be able to help the UK decarbonise by 2035\",\n",
    "    \"Additionally, US think tank Institute for Energy Economics and Financial Analysis (IEEFA) has said that SMRs are “too expensive, too slow, and too risky”.\"]\n",
    "full_text = ' '.join(text)\n",
    "\n",
    "summariser = pipeline('summarization')\n",
    "#summariser('My name is Tiril and I am happy')\n",
    "\n",
    "# Translation\n",
    "#translator = pipeline('translation', model='Helsinki-NLP/opus-mt-fr-en', max_length=100)\n",
    "#translator('Je suis une banane')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
