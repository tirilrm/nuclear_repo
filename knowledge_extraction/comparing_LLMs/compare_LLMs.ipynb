{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative LLMs:\n",
    "- LLAMA\n",
    "- Mistral\n",
    "- GPT-3.5 Turbo\n",
    "\n",
    "Non-generative LLMs:\n",
    "- BERT\n",
    "- DistilBERT\n",
    "- RoBERTa\n",
    "- ALBERT\n",
    "- XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# From huggingface\n",
    "models = {\n",
    "    'BERT': 'google-bert/bert-base-uncased',\n",
    "    'DistilBERT': 'distilbert/distilbert-base-uncased',\n",
    "    'AlBERTa': 'albert/albert-base-v2',\n",
    "    'RoBERTa': 'deepset/roberta-base-squad2',\n",
    "    'T5': 'google-t5/t5-base',\n",
    "    'XLNet': 'xlnet/xlnet-base-cased'\n",
    "}\n",
    "tokenizer = models['BERT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/tiril/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/tiril/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "# Introduction to pipeline from transfromers library\n",
    "from transformers import pipeline\n",
    "\n",
    "sentences = ['Kant was a remarkable figure',\n",
    "             'Water is made of hydrogen and oxygen',\n",
    "             'The capital of Norway is Oslo',\n",
    "             'We hope you don\\'t hate it']\n",
    "\n",
    "model = models['BERT']\n",
    "\n",
    "# Sentiment analysis\n",
    "classifier = pipeline('sentiment-analysis', model=model)\n",
    "'''for sentence in sentences:\n",
    "    print(sentence, classifier(sentence))'''\n",
    "\n",
    "\n",
    "# Zero-shot classification\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "'''for sentence in sentences:\n",
    "    print(sentence, classifier(sentence, candidate_labels=['education', 'business', 'science', 'politics']))'''\n",
    "\n",
    "# Generation\n",
    "generator = pipeline('text-generation', model=model)\n",
    "'''generator(\n",
    "    'I am a god. You are merely', \n",
    "    max_length=40, \n",
    "    num_return_sequences=5\n",
    ")'''\n",
    "\n",
    "# Fill masks\n",
    "sentence = 'The SMRs face a common issue, as <mask> they will be expensive, but the market is wide enough to trigger manufacturing processes to lower the <mask>'\n",
    "unmasker = pipeline('fill-mask', top_k=3)\n",
    "#unmasker(sentence)\n",
    "\n",
    "# NER\n",
    "ner = pipeline('ner', grouped_entities=True, model=model)\n",
    "ner('My name is Tiril Mageli and I am an MSc student at Imperial College London in the department of Computing, which is in London')\n",
    "\n",
    "# Question Answering\n",
    "qa = pipeline('question-answering')\n",
    "'''qa(\n",
    "    question='whats the weather like today?',\n",
    "    context='My name is Tiril Mageli and I am an MSc student at Imperial College London in the department of Computing, which is in London'\n",
    ")'''\n",
    "\n",
    "# Summariser\n",
    "text = [\"The six small modular reactor (SMR) developers shortlisted in Great British Nuclear’s (GBN’s) competition now have an extra two weeks to submit documentation due to the General Election.\",\n",
    "    \"The deadline for submitting project documentation has been pushed back from 24 June to 8 July. GBN said no further details were able to be shared due to restrictions on government communications during the pre-election period.\",\n",
    "    \"Energy business publication\",\n",
    "    \"reported that a request for the delay had come from one of the four US-based prospective SMR firms.\",\n",
    "    \"in October 2023 for government support to deliver a new wave of nuclear reactors are EDF Energy, GE-Hitachi Nuclear Energy International, Holtec Britain, NuScale Power, Rolls-Royce SMR and Westinghouse Electric Company UK.\",\n",
    "    \"Of those, GE-Hitachi Nuclear Energy International LLC, Holtec Britain Limited, NuScale Power and Westinghouse Electric Company UK Limited have American parent or partner companies.\",\n",
    "    \"The competition winner will receive government backing to deploy a fleet of SMRs in the UK. At the time of the competition announcement, GBN chief executive Gwyn Parry-Jones said parties would be “aiming for a final contract agreement in the summer”.\",\n",
    "    \"SMRs are nuclear power stations that have lower capacity than large-scale nuclear plantsusually in the 300MW to 500MW range. They are in theory quicker and cheaper to deploy and their construction will be easily repeatable thanks to their modular design, which will see parts created in a factory.\",\n",
    "    \"Even if not successful in GBN’s competition, many of the shortlisted firms have signalled intent to deliver SMRs in the UK.\",\n",
    "    \"announced a prototype module testing facility at the University of Sheffield\",\n",
    "    \"Holtec has shortlisted four UK sites for its SMR module factory\",\n",
    "    \". Westinghouse has plans to\",\n",
    "    \"deploy the first privately funded SMRs in North Teesside by the 2030s\",\n",
    "    \"At the time of the GBN competition announcement, energy security secretary Claire Coutinho said: “Small modular reactors will help the UK rapidly expand nuclear power; deliver cheaper, cleaner and more secure energy for British families and businesses; create well-paid, high-skilled jobs; and grow the economy.\",\n",
    "    \"“This competition has attracted designs from around the world and puts the UK at the front of the global race to develop this exciting, cutting-edge technology and cement our position as a world leader in nuclear innovation.”\",\n",
    "    \"There have been some doubts cast, with\",\n",
    "    \"the Environmental Audit Committee claiming that SMRs will not be able to help the UK decarbonise by 2035\",\n",
    "    \"Additionally, US think tank Institute for Energy Economics and Financial Analysis (IEEFA) has said that SMRs are “too expensive, too slow, and too risky”.\"]\n",
    "full_text = ' '.join(text)\n",
    "\n",
    "summariser = pipeline('summarization', model=model)\n",
    "#summariser('My name is Tiril and I am happy')\n",
    "\n",
    "# Translation\n",
    "#translator = pipeline('translation', model='Helsinki-NLP/opus-mt-fr-en', max_length=100)\n",
    "#translator('Je suis une banane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertModel\n\u001b[1;32m     35\u001b[0m bert_config \u001b[38;5;241m=\u001b[39m BertConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m \u001b[43mBertModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(bert_model)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Saving a model\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:980\u001b[0m, in \u001b[0;36mBertModel.__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mposition_embedding_type\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/transformers/modeling_utils.py:1328\u001b[0m, in \u001b[0;36mPreTrainedModel.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost_init\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;124;03m    A method executed at the end of each Transformer model initialization, to execute code that needs the model's\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;124;03m    modules properly initialized (such as weight initialization).\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1328\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_compatibility_gradient_checkpointing()\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/transformers/modeling_utils.py:2203\u001b[0m, in \u001b[0;36mPreTrainedModel.init_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpruned_heads)\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_weights:\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# Initialize weights\u001b[39;00m\n\u001b[0;32m-> 2203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# Tie weights should be skipped when not initializing all weights\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# since from_pretrained(...) calls tie weights anyways\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:894\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 894\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:894\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 894\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.apply at line 894 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:894\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 894\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:895\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    894\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m--> 895\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/transformers/modeling_utils.py:1732\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hf_initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1732\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m module\u001b[38;5;241m.\u001b[39m_is_hf_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/IndividualProject/nuclear_repo/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:836\u001b[0m, in \u001b[0;36mBertPreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the weights\"\"\"\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;66;03m# Slightly different from the TF version which uses truncated_normal for initialization\u001b[39;00m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;66;03m# cf https://github.com/pytorch/pytorch/pull/5617\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializer_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    838\u001b[0m         module\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Introduction to AutoModel from transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'albert/albert-base-v2'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "#classifier('I hate mondays')\n",
    "\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "# Loading models\n",
    "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model = AutoModel.from_pretrained('gpt2')\n",
    "#print(type(gpt_model))\n",
    "\n",
    "bart_model = AutoModel.from_pretrained('facebook/bart-base')\n",
    "#print(type(bart_model))\n",
    "\n",
    "# It is also possible to download simply the configuration (ie. random weights)\n",
    "bert_config = AutoConfig.from_pretrained('bert-base-cased')\n",
    "print(type(bert_config))\n",
    "\n",
    "# Or from a specific checkpoint\n",
    "from transformers import BertConfig\n",
    "bert_config = BertConfig.from_pretrained('bert-base-cased')\n",
    "print(type(bert_config))\n",
    "print(bert_config) # we can change any of these if we want\n",
    "\n",
    "# E.g. 10 layers instead of 12\n",
    "from transformers import BertModel\n",
    "bert_config = BertConfig.from_pretrained('bert-base-cased', num_hidden_layers=10)\n",
    "bert_model = BertModel(bert_config)\n",
    "print(bert_model)\n",
    "\n",
    "# Saving a model\n",
    "bert_model.save_pretrained('my-bert-model') #saves in current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 151, 39487, 39618, 10107, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "Tokens: ['[CLS]', 'i', 'hate', 'monday', '##s', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The AutoModelForSequenceClassification and AutoTokenizer classes work together\n",
    "to power the pipeline() used above. An AutoClass is a shortcut that automatically \n",
    "retrieves the architecture of a pretrained model.T_destination\n",
    "\n",
    "A tokenizer is responsible for preprocessing text into an array of numbers as inputs \n",
    "to a model, inluding rules for splitting at which level of word. Important: make sure\n",
    "you use the SAME tokenizer as the model was pretrained on to get good results.\n",
    "'''\n",
    "\n",
    "# Loading tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoding = tokenizer('I hate Mondays')\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "print(encoding)\n",
    "print('Tokens:', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SQuAD data (NOT NECESSARY)\n",
    "import requests\n",
    "import json \n",
    "\n",
    "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "with open('squad-dev-v2.0.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing: BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0xa948c06a0>\n",
      "Currently testing: DistilBERT\n",
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0xa97f0fa30>\n",
      "Currently testing: RoBERTa\n",
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0xa948c06a0>\n"
     ]
    }
   ],
   "source": [
    "# SQuAD test\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from datasets import load_dataset\n",
    "import difflib\n",
    "import time\n",
    "\n",
    "squad_models = {\n",
    "    'BERT': 'google-bert/bert-large-uncased-whole-word-masking-finetuned-squad',\n",
    "    'DistilBERT': 'distilbert/distilbert-base-cased-distilled-squad',\n",
    "    'RoBERTa': 'deepset/roberta-base-squad2',\n",
    "    #'TinyLLAMA': 'TinyLlama/TinyLlama-1.1B-step-50K-105b',\n",
    "    #'Mistral': 'mistralai/Mistral-7B-v0.1'\n",
    "}\n",
    "\n",
    "squad = load_dataset('squad')\n",
    "\n",
    "def is_close_match(predicted, gold_answers, threshold=0.99):\n",
    "    for gold in gold_answers:\n",
    "        similarity = difflib.SequenceMatcher(None, predicted, gold).ratio()\n",
    "        if similarity >= threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "results = {name: {} for name in squad_models}\n",
    "for name in squad_models:\n",
    "    print('Currently testing:', name)\n",
    "\n",
    "    model_name = squad_models[name]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "    print(qa_pipeline)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    for example in squad['validation']:\n",
    "        question = example['question']\n",
    "        context = example['context']\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        pred = result['answer'].strip().lower()\n",
    "        gold = [g.strip().lower() for g in example['answers']['text']]\n",
    "\n",
    "        if is_close_match(pred, gold):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    end = time.time()\n",
    "    accuracy = correct/total\n",
    "    length = end-start\n",
    "\n",
    "    results[name]['Accuracy'] = accuracy\n",
    "    results[name]['Time'] = length\n",
    "\n",
    "with open('squad_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "'''\n",
    "{\n",
    "    \"B-LOC\": 0,\n",
    "    \"B-ORG\": 1,\n",
    "    \"B-PER\": 2,\n",
    "    \"I-LOC\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"I-PER\": 5,\n",
    "    \"O\": 6\n",
    "}\n",
    "'''\n",
    "\n",
    "wikiann = load_dataset('wikiann', 'en')\n",
    "data = wikiann['test'][:]\n",
    "models = {\n",
    "    'BERT': 'dslim/bert-base-NER',\n",
    "    'DistilBERT': 'dslim/distilbert-NER',\n",
    "    'RoBERTa': 'Jean-Baptiste/roberta-large-ner-english'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens', 'ner_tags', 'langs', 'spans'])\n",
      "Tokens: ['**', 'Michael', 'Somare', ',', 'Prime', 'Minister', 'of', 'Papua', 'New', 'Guinea', '(', '1982–1985', ')']\n",
      "Tags: ['PER: Michael Somare', 'ORG: Prime Minister of Papua New Guinea']\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())\n",
    "i = 1022\n",
    "tokens = data['tokens'][i]\n",
    "spans = data['spans'][i]\n",
    "print('Tokens:', tokens)\n",
    "print('Tags:', spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(tokens):\n",
    "    string = ''\n",
    "    for token in tokens:\n",
    "        if token.isalnum():\n",
    "            string = string + ' ' + token\n",
    "        else:\n",
    "            string = string + token\n",
    "    string = string.strip()\n",
    "    return string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, wiki_dataset, length=-1):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    ner_pipeline = pipeline('ner', model=model_name, tokenizer=tokenizer)\n",
    "\n",
    "    all_preds = []\n",
    "    true_preds = []\n",
    "    \n",
    "    print('\\nCurrently testing:', model_name)\n",
    "    start_time = time.time()\n",
    "\n",
    "    tokens_all = wiki_dataset['tokens']\n",
    "    tags_all = wiki_dataset['ner_tags']\n",
    "    spans_all = wiki_dataset['spans']\n",
    "\n",
    "    iterations = len(tokens_all)\n",
    "    if length > 0:\n",
    "        iterations = length\n",
    "\n",
    "    for i in range(iterations):\n",
    "        tokens = tokens_all[i]\n",
    "        spans = spans_all[i]\n",
    "\n",
    "        text = join_tokens(tokens)\n",
    "        result = ner_pipeline(text)\n",
    "\n",
    "        print('Text:', text)\n",
    "        print('Spans:', spans)\n",
    "        print('Result:', result)\n",
    "        print('\\n')\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print('Time:', duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-LOC', 'score': 0.9938611, 'index': 1, 'word': 'Black', 'start': 0, 'end': 5}\n",
      "{'entity': 'I-LOC', 'score': 0.99812704, 'index': 2, 'word': '##town', 'start': 5, 'end': 9}\n"
     ]
    }
   ],
   "source": [
    "spans = ['ORG: Blacktown railway station']\n",
    "result = [{'entity': 'B-LOC', 'score': 0.9938611, 'index': 1, 'word': 'Black', 'start': 0, 'end': 5}, {'entity': 'I-LOC', 'score': 0.99812704, 'index': 2, 'word': '##town', 'start': 5, 'end': 9}]\n",
    "\n",
    "def test_similarity(spans, result):\n",
    "    for e in result:\n",
    "        print(e)\n",
    "\n",
    "test_similarity(spans, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently testing: dslim/bert-base-NER\n",
      "Text: Shortly afterward, an encouraging response influenced him to go to India; he arrived at Adyar in 1884.\n",
      "Spans: ['LOC: India', 'LOC: Adyar']\n",
      "Result: [{'entity': 'B-LOC', 'score': 0.99980205, 'index': 12, 'word': 'India', 'start': 67, 'end': 72}, {'entity': 'B-LOC', 'score': 0.9979063, 'index': 17, 'word': 'Ad', 'start': 88, 'end': 90}, {'entity': 'I-LOC', 'score': 0.9961746, 'index': 18, 'word': '##yar', 'start': 90, 'end': 93}]\n",
      "\n",
      "\n",
      "Text: : Kanye West featuring Jamie Foxx—`` Gold Digger''( 2005)\n",
      "Spans: ['PER: Kanye West', 'PER: Jamie Foxx', 'ORG: Gold Digger']\n",
      "Result: [{'entity': 'B-PER', 'score': 0.99878913, 'index': 2, 'word': 'Ka', 'start': 2, 'end': 4}, {'entity': 'B-PER', 'score': 0.8876877, 'index': 3, 'word': '##ny', 'start': 4, 'end': 6}, {'entity': 'I-PER', 'score': 0.6528956, 'index': 4, 'word': '##e', 'start': 6, 'end': 7}, {'entity': 'I-PER', 'score': 0.99931157, 'index': 5, 'word': 'West', 'start': 8, 'end': 12}, {'entity': 'B-PER', 'score': 0.9996362, 'index': 7, 'word': 'Jamie', 'start': 23, 'end': 28}, {'entity': 'I-PER', 'score': 0.9996275, 'index': 8, 'word': 'Fox', 'start': 29, 'end': 32}, {'entity': 'I-PER', 'score': 0.9980317, 'index': 9, 'word': '##x', 'start': 32, 'end': 33}, {'entity': 'B-MISC', 'score': 0.7681551, 'index': 13, 'word': 'Gold', 'start': 37, 'end': 41}, {'entity': 'I-MISC', 'score': 0.9940012, 'index': 14, 'word': 'Di', 'start': 42, 'end': 44}, {'entity': 'I-MISC', 'score': 0.9899536, 'index': 15, 'word': '##gger', 'start': 44, 'end': 48}]\n",
      "\n",
      "\n",
      "Text: Blacktown railway station\n",
      "Spans: ['ORG: Blacktown railway station']\n",
      "Result: [{'entity': 'B-LOC', 'score': 0.9938611, 'index': 1, 'word': 'Black', 'start': 0, 'end': 5}, {'entity': 'I-LOC', 'score': 0.99812704, 'index': 2, 'word': '##town', 'start': 5, 'end': 9}]\n",
      "\n",
      "\n",
      "Text: '' Mycalesis perseus lalassis''( Hewitson, 1864)\n",
      "Spans: ['LOC: Mycalesis perseus lalassis']\n",
      "Result: [{'entity': 'B-MISC', 'score': 0.8663102, 'index': 3, 'word': 'My', 'start': 3, 'end': 5}, {'entity': 'I-MISC', 'score': 0.38125262, 'index': 4, 'word': '##cale', 'start': 5, 'end': 9}, {'entity': 'B-PER', 'score': 0.99717855, 'index': 15, 'word': 'He', 'start': 33, 'end': 35}, {'entity': 'B-PER', 'score': 0.78177184, 'index': 16, 'word': '##wi', 'start': 35, 'end': 37}]\n",
      "\n",
      "\n",
      "Text: Jonny Lee Miller- Eli Stone''\n",
      "Spans: ['PER: Jonny Lee Miller', 'ORG: Eli Stone']\n",
      "Result: [{'entity': 'B-PER', 'score': 0.99943537, 'index': 1, 'word': 'Jon', 'start': 0, 'end': 3}, {'entity': 'I-PER', 'score': 0.56928086, 'index': 2, 'word': '##ny', 'start': 3, 'end': 5}, {'entity': 'I-PER', 'score': 0.9984956, 'index': 3, 'word': 'Lee', 'start': 6, 'end': 9}, {'entity': 'I-PER', 'score': 0.9988721, 'index': 4, 'word': 'Miller', 'start': 10, 'end': 16}, {'entity': 'B-PER', 'score': 0.99932265, 'index': 6, 'word': 'Eli', 'start': 18, 'end': 21}, {'entity': 'I-PER', 'score': 0.99907875, 'index': 7, 'word': 'Stone', 'start': 22, 'end': 27}]\n",
      "\n",
      "\n",
      "Text: Tambourissa cocottensis'' Lorence\n",
      "Spans: ['LOC: Tambourissa cocottensis']\n",
      "Result: [{'entity': 'B-MISC', 'score': 0.42747232, 'index': 1, 'word': 'Tam', 'start': 0, 'end': 3}, {'entity': 'I-MISC', 'score': 0.77274495, 'index': 4, 'word': '##ssa', 'start': 8, 'end': 11}, {'entity': 'B-PER', 'score': 0.5418475, 'index': 10, 'word': 'Lo', 'start': 26, 'end': 28}]\n",
      "\n",
      "\n",
      "Text: Admission to the bar in the United States\n",
      "Spans: ['ORG: Admission to the bar in the United States']\n",
      "Result: [{'entity': 'B-LOC', 'score': 0.99957865, 'index': 8, 'word': 'United', 'start': 28, 'end': 34}, {'entity': 'I-LOC', 'score': 0.9990754, 'index': 9, 'word': 'States', 'start': 35, 'end': 41}]\n",
      "\n",
      "\n",
      "Time: 0.3593878746032715\n"
     ]
    }
   ],
   "source": [
    "test_model(models['BERT'], data, length=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
