{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.insert(0, '../comparing_LLMs/')\n",
    "from _NER import join_tokens, merge_result, get_predicted_tags, calculate_metrics\n",
    "\n",
    "articles = pd.read_json('../../scraping/articles/all_articles.json')\n",
    "text = articles['text'][45]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Joined:\n",
      "B-ORG : Department for Energy Security and Net Zero\n",
      "B-ORG : DESNZ\n",
      "B-ORG : Great British Nuclear\n",
      "B-ORG : GBN\n",
      "B-LOC : UK\n"
     ]
    }
   ],
   "source": [
    "model = \"dslim/distilbert-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "ner_pipeline = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = [articles['text'][45][0]]\n",
    "\n",
    "total_result = []\n",
    "for paragrpah in text:\n",
    "    result = ner_pipeline(paragrpah)\n",
    "    for r in result:\n",
    "        total_result.append(r)\n",
    "\n",
    "merged_total = merge_result(total_result, model)\n",
    "\n",
    "def merge_entities(entities):\n",
    "    merged_entities = []\n",
    "    labels = []\n",
    "    current = None\n",
    "\n",
    "    for entity in entities:\n",
    "        if current is None:\n",
    "            current = entity\n",
    "            labels.append(entity['entity'][2:])\n",
    "        else:\n",
    "            if entity['entity'].startswith('I') and current['entity'].startswith('B'):\n",
    "                current['word'] += ' ' + entity['word']\n",
    "                current['end'] = entity['end']\n",
    "                current['score'] = min(current['score'], entity['score'])\n",
    "                labels.append(entity['entity'][2:])\n",
    "            else:\n",
    "                dominant_label = Counter(labels).most_common(1)[0][0]\n",
    "                current['entity'] = 'B-' + dominant_label\n",
    "                merged_entities.append(current)\n",
    "                labels = [entity['entity'][2:]]\n",
    "                current = entity\n",
    "\n",
    "    if current is not None:\n",
    "        dominant_label = Counter(labels).most_common(1)[0][0]\n",
    "        current['entity'] = 'B-' + dominant_label\n",
    "        merged_entities.append(current)\n",
    "    \n",
    "    return merged_entities\n",
    "\n",
    "print('\\nJoined:')\n",
    "joined_result = merge_entities(merged_total)\n",
    "for r in joined_result:\n",
    "    print(r['entity'],':',r['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><triplet> Department for Energy Security and Net Zero <subj> Great British Nuclear <obj> subsidiary <triplet> Great British Nuclear <subj> Department for Energy Security and Net Zero <obj> parent organization</s>\n",
      "[{'head': 'Department for Energy Security and Net Zero', 'type': 'subsidiary', 'tail': 'Great British Nuclear'}, {'head': 'Great British Nuclear', 'type': 'parent organization', 'tail': 'Department for Energy Security and Net Zero'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model = 'Babelscape/rebel-large'\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model=model, tokenizer=model)\n",
    "\n",
    "text = articles['text'][45][0]\n",
    "\n",
    "# CODE FROM DOCUMENTATION\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "print(extracted_text[0])\n",
    "\n",
    "extracted_triplets = extract_triplets(extracted_text[0])\n",
    "print(extracted_triplets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
