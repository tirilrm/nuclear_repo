{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = [c.strip() for c in examples[\"context\"]]\n",
    "    return tokenizer(questions, contexts, truncation=True, padding=\"max_length\", max_length=384)\n",
    "\n",
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=['id', 'title', 'context', 'question', 'answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "eval_dataset = tokenized_squad[\"validation\"]\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answers(start_logits, end_logits, batch, tokenizer):\n",
    "    answers = []\n",
    "    for i in range(len(start_logits)):\n",
    "        start_idx = torch.argmax(start_logits[i]).item()\n",
    "        end_idx = torch.argmax(end_logits[i]).item()\n",
    "        if start_idx <= end_idx:\n",
    "            answer = tokenizer.decode(batch['input_ids'][i][start_idx:end_idx+1], skip_special_tokens=True)\n",
    "        else:\n",
    "            answer = \"\"\n",
    "        answers.append(answer)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "example_ids = squad[\"validation\"][\"id\"]\n",
    "for batch in eval_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "        outputs = model(**inputs)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "        # Extract answers\n",
    "        batch_predictions = extract_answers(start_logits, end_logits, batch, tokenizer)\n",
    "        all_predictions.extend(batch_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tiril/Documents/IndividualProject/nuclear_repo/evaluate-v2.0.py\", line 276, in <module>\n",
      "    main()\n",
      "  File \"/Users/tiril/Documents/IndividualProject/nuclear_repo/evaluate-v2.0.py\", line 232, in main\n",
      "    with open(OPTS.data_file) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'squad/dev-v2.0.json'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'evaluate-v2.0.py', 'squad/dev-v2.0.json', 'predictions.json'], returncode=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "predictions = {id_: answer for id_, answer in zip(example_ids, all_predictions)}\n",
    "with open(\"predictions.json\", \"w\") as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "# Download the SQuAD evaluation script if you haven't already\n",
    "# wget https://raw.githubusercontent.com/allenai/bi-att-flow/master/squad/evaluate-v2.0.py\n",
    "\n",
    "# Run the evaluation script\n",
    "import subprocess\n",
    "subprocess.run([\"python\", \"evaluate-v2.0.py\", \"squad/dev-v2.0.json\", \"predictions.json\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
