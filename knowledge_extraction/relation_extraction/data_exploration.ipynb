{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset('docred', trust_remote_code=True)\n",
    "annotated = pd.DataFrame(dataset['train_annotated'])\n",
    "distant = pd.DataFrame(dataset['train_distant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Skai', 'TV', 'is', 'a', 'Greek', 'free', '-', 'to', '-', 'air', 'television', 'network', 'based', 'in', 'Piraeus', '.'], ['It', 'is', 'part', 'of', 'the', 'Skai', 'Group', ',', 'one', 'of', 'the', 'largest', 'media', 'groups', 'in', 'the', 'country', '.'], ['It', 'was', 'relaunched', 'in', 'its', 'present', 'form', 'on', '1st', 'of', 'April', '2006', 'in', 'the', 'Athens', 'metropolitan', 'area', ',', 'and', 'gradually', 'spread', 'its', 'coverage', 'nationwide', '.'], ['Besides', 'digital', 'terrestrial', 'transmission', ',', 'it', 'is', 'available', 'on', 'the', 'subscription', '-', 'based', 'encrypted', 'services', 'of', 'Nova', 'and', 'Cosmote', 'TV', '.'], ['Skai', 'TV', 'is', 'also', 'a', 'member', 'of', 'Digea', ',', 'a', 'consortium', 'of', 'private', 'television', 'networks', 'introducing', 'digital', 'terrestrial', 'transmission', 'in', 'Greece', '.'], ['At', 'launch', ',', 'Skai', 'TV', 'opted', 'for', 'dubbing', 'all', 'foreign', 'language', 'content', 'into', 'Greek', ',', 'instead', 'of', 'using', 'subtitles', '.'], ['This', 'is', 'very', 'uncommon', 'in', 'Greece', 'for', 'anything', 'except', 'documentaries', '(', 'using', 'voiceover', 'dubbing', ')', 'and', 'children', \"'s\", 'programmes', '(', 'using', 'lip', '-', 'synced', 'dubbing', ')', ',', 'so', 'after', 'intense', 'criticism', 'the', 'station', 'switched', 'to', 'using', 'subtitles', 'for', 'almost', 'all', 'foreign', 'shows', '.']]\n",
      "[['Washington', 'Place', '(', 'William', 'Washington', 'House', ')', 'is', 'one', 'of', 'the', 'first', 'homes', 'built', 'by', 'freed', 'slaves', 'after', 'the', 'Emancipation', 'Proclamation', 'of', '1863', 'in', 'Hampshire', 'County', ',', 'West', 'Virginia', ',', 'United', 'States', '.'], ['Washington', 'Place', 'was', 'built', 'by', 'William', 'and', 'Annie', 'Washington', 'in', 'north', 'Romney', 'between', '1863', 'and', '1874', 'on', 'land', 'given', 'to', 'Annie', 'by', 'her', 'former', 'owner', ',', 'Susan', 'Blue', 'Parsons', 'of', 'Wappocomo', 'plantation', '.'], ['William', 'Washington', 'later', 'acquired', 'other', 'properties', 'on', 'the', 'hills', 'north', 'of', 'Romney', 'along', 'West', 'Virginia', 'Route', '28', 'and', 'became', 'the', 'first', 'African', '-', 'American', 'land', 'developer', 'in', 'the', 'state', 'of', 'West', 'Virginia', '.'], ['One', 'of', 'his', 'subdivisions', 'is', 'the', '\"', 'Blacks', 'Hill', '\"', 'neighborhood', 'of', 'Romney', ',', 'adjacent', 'to', 'the', 'Washington', 'Place', 'homestead', '.'], ['Washington', 'Place', 'was', 'bought', 'and', 'restored', 'by', 'Ralph', 'W.', 'Haines', ',', 'a', 'local', 'attorney', 'and', 'historic', 'preservationist', '.']]\n",
      "[['IBM', 'Research', '–', 'Brazil', 'is', 'one', 'of', 'twelve', 'research', 'laboratories', 'comprising', 'IBM', 'Research', ',', 'its', 'first', 'in', 'South', 'America', '.'], ['It', 'was', 'established', 'in', 'June', '2010', ',', 'with', 'locations', 'in', 'São', 'Paulo', 'and', 'Rio', 'de', 'Janeiro', '.'], ['Research', 'focuses', 'on', 'Industrial', 'Technology', 'and', 'Science', ',', 'Systems', 'of', 'Engagement', 'and', 'Insight', ',', 'Social', 'Data', 'Analytics', 'and', 'Natural', 'Resources', 'Solutions', '.'], ['The', 'new', 'lab', ',', 'IBM', \"'s\", 'ninth', 'at', 'the', 'time', 'of', 'opening', 'and', 'first', 'in', '12', 'years', ',', 'underscores', 'the', 'growing', 'importance', 'of', 'emerging', 'markets', 'and', 'the', 'globalization', 'of', 'innovation', '.'], ['In', 'collaboration', 'with', 'Brazil', \"'s\", 'government', ',', 'it', 'will', 'help', 'IBM', 'to', 'develop', 'technology', 'systems', 'around', 'natural', 'resource', 'development', 'and', 'large', '-', 'scale', 'events', 'such', 'as', 'the', '2016', 'Summer', 'Olympics', '.'], ['Engineer', 'and', 'associate', 'lab', 'director', 'Ulisses', 'Mello', 'explains', 'that', 'IBM', 'has', 'four', 'priority', 'areas', 'in', 'Brazil', ':', '\"', 'The', 'main', 'area', 'is', 'related', 'to', 'natural', 'resources', 'management', ',', 'involving', 'oil', 'and', 'gas', ',', 'mining', 'and', 'agricultural', 'sectors', '.'], ['The', 'second', 'is', 'the', 'social', 'data', 'analytics', 'segment', 'that', 'comprises', 'the', 'analysis', 'of', 'data', 'generated', 'from', 'social', 'networking', 'sites', '[', 'such', 'as', 'Twitter', 'or', 'Facebook', ']', ',', 'which', 'can', 'be', 'applied', ',', 'for', 'example', ',', 'to', 'financial', 'analysis', '.'], ['The', 'third', 'strategic', 'area', 'is', 'nanotechnology', 'applied', 'to', 'the', 'development', 'of', 'the', 'smarter', 'devices', 'for', 'the', 'intermittent', 'production', 'industry', '.'], ['This', 'technology', 'can', 'be', 'applied', 'to', ',', 'for', 'example', ',', 'blood', 'testing', 'or', 'recovering', 'oil', 'from', 'existing', 'fields', '.'], ['And', 'the', 'last', 'one', 'is', 'smarter', 'cities', '.', '\"']]\n"
     ]
    }
   ],
   "source": [
    "validation = pd.DataFrame(dataset['validation'])\n",
    "\n",
    "for i in range(3):\n",
    "    print(validation.iloc[i]['sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'move_to_root' from 'prepare_data' (/Users/tiril/Documents/nuclear_repo/knowledge_extraction/relation_extraction/prepare_data.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m ner_model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprepare_data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprepare_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m identify_entities, create_entity_pairs, get_keywords, move_to_root\n\u001b[1;32m     12\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(prepare_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprepare_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m identify_entities, create_entity_pairs, get_keywords, move_to_root\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'move_to_root' from 'prepare_data' (/Users/tiril/Documents/nuclear_repo/knowledge_extraction/relation_extraction/prepare_data.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from _RE import join_text\n",
    "import pickle\n",
    "import importlib\n",
    "\n",
    "model_name = 'dslim/distilbert-NER'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "import prepare_data\n",
    "from prepare_data import identify_entities, create_entity_pairs, get_keywords, move_to_root\n",
    "importlib.reload(prepare_data)\n",
    "from prepare_data import identify_entities, create_entity_pairs, get_keywords, move_to_root\n",
    "\n",
    "ner_pipeline = pipeline('ner', model=ner_model, tokenizer=tokenizer)\n",
    "\n",
    "def make_paragraphs(sents):\n",
    "    paragraphs = []\n",
    "    for word_list in sents:\n",
    "        paragraph = ' '.join(word_list)\n",
    "        paragraphs.append(paragraph)\n",
    "    return paragraphs\n",
    "\n",
    "datasets = [\n",
    "    'validation',\n",
    "    'test',\n",
    "    'train_annotated',\n",
    "    'train_distant'\n",
    "]\n",
    "\n",
    "custom_keywords = get_keywords()\n",
    "train_test_data = {}\n",
    "\n",
    "for datatype in datasets:\n",
    "    print('Now doing:', datatype)\n",
    "    ds = pd.DataFrame(dataset[datatype])\n",
    "    context_and_pairs = []\n",
    "    length = len(ds)\n",
    "    length = 10\n",
    "\n",
    "    for i in range(length):\n",
    "        print(f\"{(i/length)*100:.3f}%\")\n",
    "        elems = {}\n",
    "        sents = ds.iloc[i]['sents']\n",
    "        paragraphs = make_paragraphs(sents)\n",
    "        entities = identify_entities(paragraphs, custom_keywords)\n",
    "        pairs = create_entity_pairs(entities)\n",
    "        elems['context'] = join_text(sents, fancy=False)\n",
    "        elems['pairs'] = pairs\n",
    "        context_and_pairs.append(elems)\n",
    "    \n",
    "    train_test_data[datatype] = context_and_pairs\n",
    "    \n",
    "move_to_root()\n",
    "with open('knowledge_extraction/relation_extraction/data/docred_context_and_pairs.pkl', 'wb') as file:\n",
    "    pickle.dump(context_and_pairs, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied directly for simplicity\n",
    "custom_keywords = {'FUEL': ['u235', 'u238', 'uranium-235', 'uranium-238uranium compound', 'uranium oxide', 'uranium dioxideuranium fuel', 'nuclear fuel', 'mox', 'mox fuel', 'mixed oxide fuel', 'plutonium', 'pu239', 'plutonium-239', 'thorium', 'actinides', 'light water', 'heavy water'], 'FUEL_CYCLE': ['uranium oxide', 'uranium hexafluoride', 'hex', 'wet process', 'dry process', 'uranium enrichment', 'gas centrifuge', 'fuel rods', 'fuel assembly', 'low enriched fuel', 'leu', 'highly enriched fuel', 'heu', 'high assay low enriched uranium', 'haleu', 'triso', 'spent fuel', 'spent nuclear fuel', 'nuclear waste', 'radioactive waste', 'spent oxide fuel', 'spent reactor fuel', 'spent fuel pools', 'spent fuel ponds'], 'SMR_DESIGN': ['water-cooled', 'water cooled', 'light water reactor', 'lwr', 'heavy water reactor', 'hwr', 'boiling water reactor', 'pressurized water reactor', 'pwr', 'high temperature gas reactor', 'htgr', 'gas reactor', 'gas-cooled', 'gas cooled', 'pebble bed reactor', 'pbmrliquid metal cooled', 'liquid-metal-cooled', 'liquid metal-cooled', 'lead-bismuth', 'sodium cooled', 'sodium-cooledmolten salt reactor', 'molten salt', 'msrmicroreactor', 'micro reactormicro modular reactor', 'micro nuclear reactor'], 'REACTOR': ['nuclear reactor', 'nuclear power plant', 'nuclear power reactor', 'fast reactor'], 'SMR': ['smr', 'small modular reactor', 'small nuclear reactor'], 'POLITICAL': ['safety', 'security', 'nuclear regulation', 'proliferation', 'safeguards']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'sents', 'vertexSet', 'labels'],\n",
      "        num_rows: 998\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'sents', 'vertexSet', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train_annotated: Dataset({\n",
      "        features: ['title', 'sents', 'vertexSet', 'labels'],\n",
      "        num_rows: 3053\n",
      "    })\n",
      "    train_distant: Dataset({\n",
      "        features: ['title', 'sents', 'vertexSet', 'labels'],\n",
      "        num_rows: 101873\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Format:\n",
    "(source: https://github.com/thunlp/DocRED/blob/master/data/README.md)\n",
    "{\n",
    "  'title',\n",
    "  'sents':     [\n",
    "                  [word in sent 0], # list of lists of words forming sentences\n",
    "                  [word in sent 1]\n",
    "               ]\n",
    "  'vertexSet': [\n",
    "                  [\n",
    "                    { 'name': mention_name, # name of entity mention\n",
    "                      'sent_id': mention in which sentence, --> index of the sentence where the mention occurs\n",
    "                      'pos': postion of mention in a sentence,  --> start and end position (indices) of the mention in the sentence\n",
    "                      'type': NER_type} #the NER type, e.g. PERSON, LOCATION\n",
    "                    {anthor mention}\n",
    "                  ], \n",
    "                  [anthoer entity]\n",
    "                ]\n",
    "  'labels':   [\n",
    "                {\n",
    "                  'h': idx of head entity in vertexSet,\n",
    "                  't': idx of tail entity in vertexSet,\n",
    "                  'r': relation,\n",
    "                  'evidence': evidence sentences' id --> the sentences from which the relation is supported\n",
    "                }\n",
    "              ]\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zest Airways , Inc. operated as AirAsia Zest ( formerly Asian Spirit and Zest Air ) , was a low - cost airline based at the Ninoy Aquino International Airport in Pasay City , Metro Manila in the Philippines .\n",
      "It operated scheduled domestic and international tourist services , mainly feeder services linking Manila and Cebu with 24 domestic destinations in support of the trunk route operations of other airlines .\n",
      "In 2013 , the airline became an affiliate of Philippines AirAsia operating their brand separately .\n",
      "Its main base was Ninoy Aquino International Airport , Manila .\n",
      "The airline was founded as Asian Spirit , the first airline in the Philippines to be run as a cooperative .\n",
      "On August 16 , 2013 , the Civil Aviation Authority of the Philippines ( CAAP ) , the regulating body of the Government of the Republic of the Philippines for civil aviation , suspended Zest Air flights until further notice because of safety issues .\n",
      "Less than a year after AirAsia and Zest Air 's strategic alliance , the airline has been rebranded as AirAsia Zest .\n",
      "The airline was merged into AirAsia Philippines in January 2016 .\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import _RE\n",
    "importlib.reload(_RE)\n",
    "from _RE import join_text, make_triplets\n",
    "\n",
    "sents = annotated.iloc[0]['sents']\n",
    "text = join_text(sents, fancy=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mississippi River'], ['P131', 'located in the administrative territorial entity'], ['Illinois']]\n",
      "[['Mississippi River'], ['P17', 'country'], ['United States']]\n",
      "[['Madison County'], ['P131', 'located in the administrative territorial entity'], ['Illinois']]\n",
      "[['Madison County'], ['P17', 'country'], ['United States']]\n",
      "[['Illinois'], ['P206', 'located in or next to body of water'], ['Mississippi River']]\n",
      "[['Illinois'], ['P150', 'contains administrative territorial entity'], ['Madison County']]\n",
      "[['Illinois'], ['P131', 'located in the administrative territorial entity'], ['United States']]\n",
      "[['Illinois'], ['P17', 'country'], ['United States']]\n",
      "[['United States'], ['P150', 'contains administrative territorial entity'], ['Illinois']]\n",
      "[['United States'], ['P150', 'contains administrative territorial entity'], ['Missouri']]\n",
      "[['Missouri'], ['P131', 'located in the administrative territorial entity'], ['United States']]\n",
      "[['Missouri'], ['P17', 'country'], ['United States']]\n",
      "[['Greater St. Louis', 'St. Louis'], ['P17', 'country'], ['United States']]\n",
      "[['American Civil War', 'Civil War'], ['P17', 'country'], ['United States']]\n",
      "[['American Civil War', 'Civil War'], ['P495', 'country of origin'], ['United States']]\n",
      "[['Miles Davis'], ['P27', 'country of citizenship'], ['United States']]\n",
      "[['Miles Davis'], ['P19', 'place of birth'], ['Alton', 'Alton']]\n",
      "[['Robert Wadlow'], ['P27', 'country of citizenship'], ['United States']]\n",
      "[['Robert Wadlow'], ['P19', 'place of birth'], ['Alton', 'Alton']]\n",
      "[['Robert Wadlow'], ['P551', 'residence'], ['Alton', 'Alton']]\n",
      "[['Metro - East'], ['P17', 'country'], ['United States']]\n",
      "[['Alton', 'Alton'], ['P131', 'located in the administrative territorial entity'], ['Madison County']]\n",
      "[['Alton', 'Alton'], ['P17', 'country'], ['United States']]\n"
     ]
    }
   ],
   "source": [
    "index = 6\n",
    "labels = annotated.iloc[index]['labels']\n",
    "vertexSet = annotated['vertexSet'][index]\n",
    "\n",
    "'''print(annotated.columns)\n",
    "for subset in vertexSet:\n",
    "    print (subset)\n",
    "for l in labels:\n",
    "    print(l, labels[l])'''\n",
    "\n",
    "triplet = make_triplets(vertexSet, labels)\n",
    "\n",
    "for t in triplet:\n",
    "    print(t)\n",
    "\n",
    "# The first triplet here makes no sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filenames = [\n",
    "    #'docred_metadata/char2id.json',\n",
    "    #'docred_metadata/ner2id.json',\n",
    "    #'docred_metadata/rel2id.json',\n",
    "    #'docred_metadata/word2id.json',\n",
    "    'docred_metadata/rel_info.json'\n",
    "    ]\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 96\n",
      "1. P6: head of government\n",
      "2. P17: country\n",
      "3. P19: place of birth\n",
      "4. P20: place of death\n",
      "5. P22: father\n",
      "6. P25: mother\n",
      "7. P26: spouse\n",
      "8. P27: country of citizenship\n",
      "9. P30: continent\n",
      "10. P31: instance of\n",
      "11. P35: head of state\n",
      "12. P36: capital\n",
      "13. P37: official language\n",
      "14. P39: position held\n",
      "15. P40: child\n",
      "16. P50: author\n",
      "17. P54: member of sports team\n",
      "18. P57: director\n",
      "19. P58: screenwriter\n",
      "20. P69: educated at\n",
      "21. P86: composer\n",
      "22. P102: member of political party\n",
      "23. P108: employer\n",
      "24. P112: founded by\n",
      "25. P118: league\n",
      "26. P123: publisher\n",
      "27. P127: owned by\n",
      "28. P131: located in the administrative territorial entity\n",
      "29. P136: genre\n",
      "30. P137: operator\n",
      "31. P140: religion\n",
      "32. P150: contains administrative territorial entity\n",
      "33. P155: follows\n",
      "34. P156: followed by\n",
      "35. P159: headquarters location\n",
      "36. P161: cast member\n",
      "37. P162: producer\n",
      "38. P166: award received\n",
      "39. P170: creator\n",
      "40. P171: parent taxon\n",
      "41. P172: ethnic group\n",
      "42. P175: performer\n",
      "43. P176: manufacturer\n",
      "44. P178: developer\n",
      "45. P179: series\n",
      "46. P190: sister city\n",
      "47. P194: legislative body\n",
      "48. P205: basin country\n",
      "49. P206: located in or next to body of water\n",
      "50. P241: military branch\n",
      "51. P264: record label\n",
      "52. P272: production company\n",
      "53. P276: location\n",
      "54. P279: subclass of\n",
      "55. P355: subsidiary\n",
      "56. P361: part of\n",
      "57. P364: original language of work\n",
      "58. P400: platform\n",
      "59. P403: mouth of the watercourse\n",
      "60. P449: original network\n",
      "61. P463: member of\n",
      "62. P488: chairperson\n",
      "63. P495: country of origin\n",
      "64. P527: has part\n",
      "65. P551: residence\n",
      "66. P569: date of birth\n",
      "67. P570: date of death\n",
      "68. P571: inception\n",
      "69. P576: dissolved, abolished or demolished\n",
      "70. P577: publication date\n",
      "71. P580: start time\n",
      "72. P582: end time\n",
      "73. P585: point in time\n",
      "74. P607: conflict\n",
      "75. P674: characters\n",
      "76. P676: lyrics by\n",
      "77. P706: located on terrain feature\n",
      "78. P710: participant\n",
      "79. P737: influenced by\n",
      "80. P740: location of formation\n",
      "81. P749: parent organization\n",
      "82. P800: notable work\n",
      "83. P807: separated from\n",
      "84. P840: narrative location\n",
      "85. P937: work location\n",
      "86. P1001: applies to jurisdiction\n",
      "87. P1056: product or material produced\n",
      "88. P1198: unemployment rate\n",
      "89. P1336: territory claimed by\n",
      "90. P1344: participant of\n",
      "91. P1365: replaces\n",
      "92. P1366: replaced by\n",
      "93. P1376: capital of\n",
      "94. P1412: languages spoken, written or signed\n",
      "95. P1441: present in work\n",
      "96. P3373: sibling\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open ('docred_metadata/rel_info.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('Number of classes:', len(data))\n",
    "for i, key in enumerate(data):\n",
    "    print(f\"{i+1}. {key}: {data[key]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
